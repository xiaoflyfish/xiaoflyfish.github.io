---
title: 分词
categories: 
- ElasticSearch
---

将文本转换成一系列单词Term/Token的过程，也可称作文本分析，ES中叫作：**Analysis**。

**分词器(Analyzer)：**

ES中专门处理分词的组件，**组成**和**执行顺序**如下：

* Character Filters(多个)。针对原始文本进行处理，如：去除html特使标记符`</p>`等。

* Tokenizer(一个)。将原始文本按一定规则划分为单词。

* Token Filters(多个)。针对Tokenizer处理的单词进行再加工，如：转小写、删除、新增等。

**分词API：**

ES提供了一个测试分词的API接口，使用`endpoint：_analyze`。

并且可以指定分词器进行测试，还可以直接指定索引中的字段，甚至自定义分词器进行测试。

指定分词器：

```json
#指定分词器进行分词测试
POST _analyze
{
 "analyzer":"standard",
 "text":"hello world!"
}
```

直接指定索引中字段：

```json
#直接指定索引中字段
POST test_index/_analyze
{
 "field":"username",
 "text":"hello world!"
}
```

自定义分词器：

```json
#自定义分词器，自定义Tokenizer、filter、等进行分词，举例：
POST _analyze
{
 "tokenizer":"standard",
 "filter":["lowercase"],
 "text":"Hello World!"
}
```

**ES自带分词器：**

* standard(默认分词器)：按词划分、支持多语言、小写处理。

**中文分词器：**

目前，常用的中文分词器有：

* IK。实现中英文分词，支持多模式，可自定义词库，支持热更新分词词典。

* jieba。python中流行，支持繁体分词、并行分词，可自定义词典、词性标记等。