---
title: HashMap
categories: 
- Java基础
- 集合类
---

# 整体架构

HashMap 底层的数据结构主要是：数组 + 链表 + 红黑树。其中当链表的长度大于等于 8 时，链表会转化成红黑树，当红黑树的大小小于等于 6 时，红黑树会转化成链表，整体的数据结构如下：

![](https://xiaoflyfish.oss-cn-beijing.aliyuncs.com/image/20201127095949.png)

# 新增

新增 key，value 大概的步骤如下：

1. 空数组有无初始化，没有的话初始化；
2. 如果通过 key 的 hash 能够直接找到值，跳转到 6，否则到 3；
3. 如果 hash 冲突，两种解决方案：链表 or 红黑树；
4. 如果是链表，递归循环，把新元素追加到队尾；
5. 如果是红黑树，调用红黑树新增的方法；
6. 通过 2、4、5 将新元素追加成功，再根据 onlyIfAbsent 判断是否需要覆盖；
7. 判断是否需要扩容，需要扩容进行扩容，结束。

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/053f9916a958437880f1abeff91b1205~tplv-k3u1fbpfcp-watermark.image)

**为什么是 8，这个答案在源码中注释有说，中文翻译过来大概的意思是：**

链表查询的时间复杂度是 O (n)，红黑树的查询复杂度是 O (log (n))。在链表数据不多的时候，使用链表进行遍历也比较快，只有当链表数据比较多的时候，才会转化成红黑树，但红黑树需要的占用空间是链表的 2 倍，考虑到转化时间和空间损耗，所以我们需要定义出转化的边界值。

在考虑设计 8 这个值的时候，我们参考了泊松分布概率函数，由泊松分布中得出结论，链表各个长度的命中概率为：

```
* 0:    0.60653066
* 1:    0.30326533
* 2:    0.07581633
* 3:    0.01263606
* 4:    0.00157952
* 5:    0.00015795
* 6:    0.00001316
* 7:    0.00000094
* 8:    0.00000006
```

意思是，当链表的长度是 8 的时候，出现的概率是 0.00000006，不到千万分之一，所以说正常情况下，链表的长度不可能到达 8 ，而一旦到达 8 时，肯定是 hash 算法出了问题，所以在这种情况下，为了让 HashMap 仍然有较高的查询性能，所以让链表转化成红黑树，我们正常写代码，使用 HashMap 时，几乎不会碰到链表转化成红黑树的情况，毕竟概念只有千万分之一。

# 查找

HashMap 的查找主要分为以下三步：

- 根据 hash 算法定位数组的索引位置，equals 判断当前节点是否是我们需要寻找的 key，是的话直接返回，不是的话往下。
- 判断当前节点有无 next 节点，有的话判断是链表类型，还是红黑树类型。
- 分别走链表和红黑树不同类型的查找方法。

# 扩容

**hashmap什么时候进行扩容呢？**

当hashmap中的元素个数超过数组大小*loadFactor时，就会进行数组扩容，loadFactor的默认值为0.75，也就是说，默认情况下，数组大小为16，那么当hashmap中元素个数超过16*0.75=12的时候，就把数组的大小扩展为2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知hashmap中元素的个数，那么预设元素的个数能够有效的提高hashmap的性能。

**为什么扩容是2的次幂?**
HashMap的初始容量是2的n次幂，扩容也是2倍的形式进行扩容，是因为容量是2的n次幂，可以使得添加的元素均匀分布在HashMap中的数组上，减少hash碰撞，避免形成链表的结构，使得查询效率降低！

扩容步骤

当这个数组满了以后，就会自动扩容，变成一个更大的数组，可以在里面放更多的元素。

1，在resize()方法中，定义了oldCap参数，记录了原table的长度，定义了newCap参数，记录新table长度，newCap是oldCap长度的2倍。

2，循环原table，把原table中的每个链表中的每个元素放入新table。

举个例子，假设table原长度是16，扩容后长度32，那么一个hash值在扩容前后的table下标是这么计算的：

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/39c1fed8e5174ba7bff1adc61ab3d796~tplv-k3u1fbpfcp-watermark.image)

hash值的每个二进制位用abcde来表示，那么，hash和新旧table按位与的结果，最后4位显然是相同的，唯一可能出现的区别就在第5位，也就是hash值的b所在的那一位，如果b所在的那一位是0，那么新table按位与的结果和旧table的结果就相同，反之如果b所在的那一位是1，则新table按位与的结果就比旧table的结果多了10000（二进制），而这个二进制10000就是旧table的长度16。

换言之，hash值的新散列下标是不是需要加上旧table长度，只需要看看hash值第5位是不是1就行了，位运算的方法就是hash值和10000（也就是旧table长度）来按位与，其结果只可能是10000或者00000。

所以，`e.hash & oldCap`，就是用于计算位置b到底是0还是1用的，只要其结果是0，则新散列下标就等于原散列下标，否则新散列坐标要在原散列坐标的基础上加上原table长度

# 其他问题

**JDK1.8中对hash算法优化**

JDK1.7中通过`key.hashcode()`得到关键字的hash值

hash算法的优化：

```java
//JDK1.8以后的HashMap部分源码
static final int hash(Object key){
 int h;
 return (key == null)?0(h=key.hashCode())^(h>>>16);
}
```

将32位的hash值右移16位，结果是将高16位推到了低16位上，高16位用0来补齐，优化的实质是将高16位与低16位进行异或运算，对于两个hash值的低16位相等，高16位不等的情况，尽量减少了hash冲突

**寻址算法的优化：**

寻址算法就是对长度为n的数组取模，得到在数组中的位置。根据数学规律，对n取模，就是和n-1进行与运算。与运算的效率远远高于求模运算，所以采用与运算。

当数组长度是2的幂次时，hash值对数组长度取模的效果和`hash&(n-1)`是一样的

**加载因子为什么是 0.75？**

这其实是出于容量和性能之间平衡的结果：

- 当加载因子设置比较大的时候，扩容的门槛就被提高了，扩容发生的频率比较低，占用的空间会比较小，但此时发生 Hash 冲突的几率就会提升，因此需要更复杂的数据结构来存储元素，这样对元素的操作时间就会增加，运行效率也会因此降低；
- 而当加载因子值比较小的时候，扩容的门槛会比较低，因此会占用更多的空间，此时元素的存储就比较稀疏，发生哈希冲突的可能性就比较小，因此操作性能会比较高。

所以综合了以上情况就取了一个 0.5 到 1.0 的平均数 0.75 作为加载因子。

**为什么线程不安全**

HashMap 在并发时可能出现的问题主要是两方面：

- 如果多个线程同时使用 put 方法添加元素，而且假设正好存在两个 put 的 key 发生了碰撞（根据 hash 值计算的 bucket 一样），那么根据 HashMap 的实现，这两个 key 会添加到数组的同一个位置，这样最终就会发生其中一个线程 put 的数据被覆盖
- 如果多个线程同时检测到元素个数超过数组大小 * loadFactor，这样就会发生多个线程同时对 Node 数组进行扩容，都在重新计算元素位置以及复制数据，但是最终只有一个线程扩容后的数组会赋给 table，也就是说其他线程的都会丢失，并且各自线程 put 的数据也丢失

**一般用什么作为HashMap的key?**
一般用Integer、String这种不可变类当HashMap当key，而且String最为常用。

- (1)因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。
- (2)因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的,这些类已经很规范的覆写了hashCode()以及equals()方法。

**用可变类当HashMap的key有什么问题?**
hashcode可能发生改变，导致put进去的值，无法get出，如下所示

```java
HashMap<List<String>, Object> changeMap = new HashMap<>();
List<String> list = new ArrayList<>();
list.add("hello");
Object objectValue = new Object();
changeMap.put(list, objectValue);
System.out.println(changeMap.get(list));
list.add("hello world");//hashcode发生了改变
System.out.println(changeMap.get(list));
```

输出值如下   

```
java.lang.Object@74a14482
null
```

**实现一个自定义的class作为HashMap的key该如何实现？**

记住下面四个原则
(1)两个对象相等，hashcode一定相等
(2)两个对象不等，hashcode不一定不等
(3)hashcode相等，两个对象不一定相等
(4)hashcode不等，两个对象一定不等

记住如何写一个不可变类
(1)类添加final修饰符，保证类不被继承。
如果类可以被继承会破坏类的不可变性机制，只要继承类覆盖父类的方法并且继承类可以改变成员变量值，那么一旦子类以父类的形式出现时，不能保证当前类是否可变。

(2)保证所有成员变量必须私有，并且加上final修饰
通过这种方式保证成员变量不可改变。但只做到这一步还不够，因为如果是对象成员变量有可能再外部改变其值。所以第4点弥补这个不足。

(3)不提供改变成员变量的方法，包括setter
避免通过其他接口改变成员变量的值，破坏不可变特性。

(4)通过构造器初始化所有成员，进行深拷贝(deep copy)

(5)在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝
这种做法也是防止对象外泄，防止通过getter获得内部可变成员对象后对成员变量直接操作，导致成员变量发生改变。

**和HashTable的区别**

Hashtable 是不允许键或值为 null 的，HashMap 的键值则都可以为 null

因为Hashtable在我们put 空值的时候会直接抛空指针异常，但是HashMap却做了特殊处理

HashMap如果你使用null值，就会使得其无法判断对应的key是不存在还是为空，因为你无法再调用一次

contain(key）来对key是否存在进行判断，ConcurrentHashMap同理

Hashtable 继承了 Dictionary类，而 HashMap 继承的是 AbstractMap 类，`Dictionary`的子类同时也实现了`Map`接口

HashMap 的初始容量为：16，Hashtable 初始容量为：11，两者的负载因子默认都是：0.75

当现有容量大于`总容量 * 负载因子`时，HashMap 扩容规则为当前容量翻倍，Hashtable 扩容规则为当前容量翻倍 + 1，`int newCapacity = (oldCapacity << 1) + 1;`

迭代器不同：HashMap 中的 Iterator 迭代器是 fail-fast 的，而 Hashtable 的 Enumerator 不是 fail-fast 的

当其他线程改变了HashMap 的结构，如：增加、删除元素，将会抛出ConcurrentModificationException 异常，而 Hashtable 则不会

`HashTable`是直接使用key的hashCode(`key.hashCode()`)作为hash值，不像`HashMap`内部使用`static final int hash(Object key)`扰动函数对key的hashCode进行扰动后作为hash值

`HashTable`取哈希桶下标是直接用模运算`%`（因为其默认容量也不是2的n次方。所以也无法用位运算替代模运算）

**尾插法好处**

尾插法主要是为了安全，防止环化，因为resize的赋值方式，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置，在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上

使用头插会改变链表的上的顺序，但是如果使用尾插，在扩容时会保持链表元素原本的顺序，就不会出现链表成环的问题了